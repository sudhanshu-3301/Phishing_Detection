{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "447a0ee0-a609-4e1c-a078-1dee1c2b0840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tldextract in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from tldextract) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from tldextract) (2.31.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from tldextract) (3.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
      "Requirement already satisfied: six in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from requests-file>=1.4->tldextract) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17888c2b-e3df-4e86-b5e1-a4df925564d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL 'https://secure-login.payment.tk' is likely a phishing site.\n",
      "The URL 'http://192.168.0.1/admin' is likely a phishing site.\n",
      "The URL 'https://www.example.com' is likely a phishing site.\n",
      "The URL 'http://www.bank-update.com' is likely a phishing site.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tldextract\n",
    "\n",
    "# Define a list of common phishing-related keywords\n",
    "phishing_keywords = ['login', 'secure', 'account', 'verify', 'update', 'bank', 'password', 'payment']\n",
    "\n",
    "# Define a list of known suspicious domain extensions\n",
    "suspicious_tlds = ['tk', 'ml', 'ga', 'cf', 'gq']  # Example suspicious TLDs\n",
    "\n",
    "# Define a list of known phishing domains (for demo purposes)\n",
    "known_phishing_domains = ['bad-website.tk', 'malicious-site.ga']\n",
    "\n",
    "# Function to extract features from a URL\n",
    "def extract_features(url):\n",
    "    features = {}\n",
    "\n",
    "    # Check for suspicious characters in the URL\n",
    "    features['has_at_symbol'] = '@' in url\n",
    "    features['has_double_slash'] = '//' in url.strip().lstrip('http:').lstrip('https:')\n",
    "    features['has_ip_address'] = bool(re.match(r'http[s]?://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url))\n",
    "    \n",
    "    # Extract domain and TLD\n",
    "    ext = tldextract.extract(url)\n",
    "    features['domain'] = ext.domain\n",
    "    features['subdomain'] = ext.subdomain\n",
    "    features['tld'] = ext.suffix\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to determine if a URL is phishing\n",
    "def is_phishing(url):\n",
    "    features = extract_features(url)\n",
    "\n",
    "    # Basic checks for phishing characteristics\n",
    "    if features['has_at_symbol'] or features['has_double_slash'] or features['has_ip_address']:\n",
    "        return True\n",
    "\n",
    "    # Check if the TLD is suspicious\n",
    "    if features['tld'] in suspicious_tlds:\n",
    "        return True\n",
    "\n",
    "    # Check for known phishing domains\n",
    "    full_domain = f\"{features['subdomain']}.{features['domain']}.{features['tld']}\"\n",
    "    if full_domain in known_phishing_domains:\n",
    "        return True\n",
    "\n",
    "    # Check for common phishing-related keywords in the domain or path\n",
    "    for keyword in phishing_keywords:\n",
    "        if keyword in url.lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Test the program with some example URLs\n",
    "test_urls = [\n",
    "    'https://secure-login.payment.tk',  # Suspicious domain and TLD\n",
    "    'http://192.168.0.1/admin',  # IP address\n",
    "    'https://www.example.com',  # Legitimate URL\n",
    "    'http://www.bank-update.com',  # Contains phishing keyword\n",
    "]\n",
    "\n",
    "# Check each URL to see if it's a potential phishing site\n",
    "for url in test_urls:\n",
    "    if is_phishing(url):\n",
    "        print(f\"The URL '{url}' is likely a phishing site.\")\n",
    "    else:\n",
    "        print(f\"The URL '{url}' seems safe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78e7b774-c28f-44bd-bd30-35beadc9f887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a URL to check for phishing:  https://www.facebook.com/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL 'https://www.facebook.com/' is likely a phishing site.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tldextract\n",
    "\n",
    "# Define a list of common phishing-related keywords\n",
    "phishing_keywords = ['login', 'secure', 'account', 'verify', 'update', 'bank', 'password', 'payment']\n",
    "\n",
    "# Define a list of known suspicious domain extensions\n",
    "suspicious_tlds = ['tk', 'ml', 'ga', 'cf', 'gq']  # Example suspicious TLDs\n",
    "\n",
    "# Define a list of known phishing domains (for demo purposes)\n",
    "known_phishing_domains = ['bad-website.tk', 'malicious-site.ga']\n",
    "\n",
    "# Function to extract features from a URL\n",
    "def extract_features(url):\n",
    "    features = {}\n",
    "\n",
    "    # Check for suspicious characters in the URL\n",
    "    features['has_at_symbol'] = '@' in url\n",
    "    features['has_double_slash'] = '//' in url.strip().lstrip('http:').lstrip('https:')\n",
    "    features['has_ip_address'] = bool(re.match(r'http[s]?://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url))\n",
    "    \n",
    "    # Extract domain and TLD\n",
    "    ext = tldextract.extract(url)\n",
    "    features['domain'] = ext.domain\n",
    "    features['subdomain'] = ext.subdomain\n",
    "    features['tld'] = ext.suffix\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to determine if a URL is phishing\n",
    "def is_phishing(url):\n",
    "    features = extract_features(url)\n",
    "\n",
    "    # Basic checks for phishing characteristics\n",
    "    if features['has_at_symbol'] or features['has_double_slash'] or features['has_ip_address']:\n",
    "        return True\n",
    "\n",
    "    # Check if the TLD is suspicious\n",
    "    if features['tld'] in suspicious_tlds:\n",
    "        return True\n",
    "\n",
    "    # Check for known phishing domains\n",
    "    full_domain = f\"{features['subdomain']}.{features['domain']}.{features['tld']}\"\n",
    "    if full_domain in known_phishing_domains:\n",
    "        return True\n",
    "\n",
    "    # Check for common phishing-related keywords in the domain or path\n",
    "    for keyword in phishing_keywords:\n",
    "        if keyword in url.lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Main function to run the phishing detection\n",
    "def main():\n",
    "    # Get URL input from the user\n",
    "    user_url = input(\"Please enter a URL to check for phishing: \")\n",
    "\n",
    "    # Check if the URL is likely phishing\n",
    "    if is_phishing(user_url):\n",
    "        print(f\"The URL '{user_url}' is likely a phishing site.\")\n",
    "    else:\n",
    "        print(f\"The URL '{user_url}' seems safe.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86e4b10-942e-413f-883d-4072835cac20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a URL to check for phishing:  https://altoro.testfire.net/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL 'https://altoro.testfire.net/' seems safe.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tldextract\n",
    "\n",
    "# Known phishing-related keywords (reduced list to avoid false positives)\n",
    "phishing_keywords = ['verify', 'update', 'secure']\n",
    "\n",
    "# Suspicious TLDs (remove common legitimate ones)\n",
    "suspicious_tlds = ['tk', 'ml', 'ga', 'cf', 'gq']\n",
    "\n",
    "# Known phishing domains\n",
    "known_phishing_domains = ['bad-website.tk', 'malicious-site.ga']\n",
    "\n",
    "# Function to extract features from a URL\n",
    "def extract_features(url):\n",
    "    features = {}\n",
    "\n",
    "    # Check for suspicious characters in the URL\n",
    "    features['has_at_symbol'] = '@' in url\n",
    "    features['has_double_slash'] = '//' in url.strip().lstrip('http:').lstrip('https:')\n",
    "    features['has_ip_address'] = bool(re.match(r'http[s]?://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url))\n",
    "    \n",
    "    # Extract domain and TLD\n",
    "    ext = tldextract.extract(url)\n",
    "    features['domain'] = ext.domain\n",
    "    features['subdomain'] = ext.subdomain\n",
    "    features['tld'] = ext.suffix\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to determine if a URL is phishing\n",
    "def is_phishing(url):\n",
    "    features = extract_features(url)\n",
    "\n",
    "    # Check for basic phishing characteristics\n",
    "    if features['has_at_symbol'] or features['has_ip_address']:\n",
    "        return True\n",
    "\n",
    "    # Check if the TLD is suspicious\n",
    "    if features['tld'] in suspicious_tlds:\n",
    "        return True\n",
    "\n",
    "    # Check for known phishing domains\n",
    "    full_domain = f\"{features['subdomain']}.{features['domain']}.{features['tld']}\"\n",
    "    if full_domain in known_phishing_domains:\n",
    "        return True\n",
    "\n",
    "    # Check for specific phishing-related keywords in the domain or path\n",
    "    # Avoid false positives by using a narrower keyword list\n",
    "    for keyword in phishing_keywords:\n",
    "        if keyword in url.lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Main function to run the phishing detection\n",
    "def main():\n",
    "    user_url = input(\"Please enter a URL to check for phishing: \")\n",
    "\n",
    "    if is_phishing(user_url):\n",
    "        print(f\"The URL '{user_url}' might be a phishing site.\")\n",
    "    else:\n",
    "        print(f\"The URL '{user_url}' seems safe.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9614a1ba-fd72-404d-8341-36a8afdda653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter a URL to check for phishing:  https://www.facebook.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL 'https://www.facebook.com' is likely a phishing site.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import tldextract\n",
    "\n",
    "# Define a list of common phishing-related keywords (for heuristic checks)\n",
    "phishing_keywords = ['verify', 'update', 'secure']\n",
    "\n",
    "# Define a list of suspicious TLDs\n",
    "suspicious_tlds = ['tk', 'ml', 'ga', 'cf', 'gq']  # Example suspicious TLDs\n",
    "\n",
    "# List of known phishing domains (including the new one you mentioned)\n",
    "known_phishing_domains = [\n",
    "    'att-103234.weeblysite.com',\n",
    "    'mntamasklogin.gitbook.io',\n",
    "    'anmilopbe.blogspot.co.id',\n",
    "    'ligne-orange4.godaddysites.com',\n",
    "    'telstra-108709.weeblysite.com',\n",
    "    'bt-105132.weeblysite.com',\n",
    "    'yahoo-mail-109605.weeblysite.com',\n",
    "    'bafybeiaylac7v34xccdujkx5l4ulnwfgq7nbwxux2ntsxz2hemzo75ox3y.ipfs.cf-ipfs.com',\n",
    "    'mail-109392.weeblysite.com',\n",
    "    'steveoffice.pages.dev',\n",
    "    'mako-nordic.com',\n",
    "    'generatorfreeaccounts.blogspot.my',\n",
    "    'lacasadelcuarzo.com',\n",
    "    'pub-b090674c9132409c92022d05de5e8ca4.r2.dev/index.html',\n",
    "    'quasukienff.grarena.vn',\n",
    "    'netzero-106257.weeblysite.com',\n",
    "    'document.mamabiller59.workers.dev',\n",
    "    'serverouttnethicationdomainservicesmails02.pages.dev',\n",
    "    'ow0ohrbcaz28.optimytool.com',\n",
    "    'we-zc-ash.com',\n",
    "    'terimaa-hadiiah-giveaaway.resmiii-vippp.my.id',\n",
    "    'piv-cfe.pages.dev/07e55d85-a0b3-4906-8e89-327bd8a209d9',\n",
    "    'barclaycard-pvn.de',\n",
    "    'netflix-clone-1p2.pages.dev',\n",
    "    'id-1924472.page-hotels.top/p/767242222',\n",
    "    'bafybeifadngqcxjj5qer7of2h4enrzufunfen4oolr25pixwyerf6mzjam.ipfs.cf-ipfs.com',\n",
    "    'lescondimentsdahoe.com',\n",
    "    'floral-king-ebb0.tangguayhomes.workers.dev',\n",
    "    'pub-a800c74c40594acb86f6637cf180050a.r2.dev/index.html',\n",
    "    'bafybeickmtazhjxetzymqkorn4rx24weds5qxk6fpvef2dd6e4uuwqkz3a.ipfs.cf-ipfs.com',\n",
    "]\n",
    "\n",
    "# Function to extract features from a URL\n",
    "def extract_features(url):\n",
    "    features = {}\n",
    "\n",
    "    # Check for suspicious characters\n",
    "    features['has_at_symbol'] = '@' in url\n",
    "    features['has_double_slash'] = '//' in url.strip().lstrip('http:').lstrip('https:')\n",
    "    features['has_ip_address'] = bool(re.match(r'http[s]?://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url))\n",
    "\n",
    "    # Extract domain and TLD\n",
    "    ext = tldextract.extract(url)\n",
    "    features['domain'] = ext.domain\n",
    "    features['subdomain'] = ext.subdomain\n",
    "    features['tld'] = ext.suffix\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to determine if a URL is phishing\n",
    "def is_phishing(url):\n",
    "    features = extract_features(url)\n",
    "\n",
    "    # Basic checks for phishing characteristics\n",
    "    if features['has_at_symbol'] or features['has_double_slash'] or features['has_ip_address']:\n",
    "        return True\n",
    "\n",
    "    # Check if the TLD is suspicious\n",
    "    if features['tld'] in suspicious_tlds:\n",
    "        return True\n",
    "\n",
    "    # Check if the domain is in the list of known phishing domains\n",
    "    full_domain = f\"{features['subdomain']}.{features['domain']}.{features['tld']}\"\n",
    "    if full_domain in known_phishing_domains:\n",
    "        return True\n",
    "\n",
    "    # Check for specific phishing-related keywords in the domain or path\n",
    "    for keyword in phishing_keywords:\n",
    "        if keyword in url.lower():\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Main function to run the phishing detection\n",
    "def main():\n",
    "    user_url = input(\"Please enter a URL to check for phishing: \")\n",
    "\n",
    "    if is_phishing(user_url):\n",
    "        print(f\"The URL '{user_url}' is likely a phishing site.\")\n",
    "    else:\n",
    "        print(f\"The URL '{user_url}' seems safe.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b92e0238-9c6b-4c37-a657-390d5bd9d70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: tldextract in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from tldextract) (3.4)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from tldextract) (2.31.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from tldextract) (3.13.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sudhanshu kumar\\anaconda3\\lib\\site-packages (from requests>=2.1.0->tldextract) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628dbbf2-c991-4aee-b91e-cd1100ef760b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'URL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'URL'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_phishing.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Extract features for each URL in the dataset\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mURL\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_features)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Split features and labels\u001b[39;00m\n\u001b[0;32m     36\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())  \u001b[38;5;66;03m# Features are in list form\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'URL'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tldextract\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to extract features from a URL\n",
    "def extract_features(url):\n",
    "    features = {}\n",
    "\n",
    "    # Check for suspicious characters\n",
    "    features['has_at_symbol'] = '@' in url\n",
    "    features['has_double_slash'] = '//' in url.strip().lstrip('http:').lstrip('https:')\n",
    "    features['has_ip_address'] = bool(re.match(r'http[s]?://\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', url))\n",
    "\n",
    "    # Extract domain and TLD\n",
    "    ext = tldextract.extract(url)\n",
    "    features['domain_length'] = len(ext.domain)\n",
    "    features['subdomain_count'] = len(ext.subdomain.split('.'))\n",
    "    features['tld'] = ext.suffix\n",
    "\n",
    "    # Check for common phishing-related keywords\n",
    "    phishing_keywords = ['verify', 'update', 'secure', 'login', 'account']\n",
    "    features['contains_keyword'] = any(keyword in url.lower() for keyword in phishing_keywords)\n",
    "\n",
    "    return list(features.values())\n",
    "\n",
    "# Load data from CSV\n",
    "data = pd.read_csv('dataset_phishing.csv')\n",
    "\n",
    "# Extract features for each URL in the dataset\n",
    "data['features'] = data['URL'].apply(extract_features)\n",
    "\n",
    "# Split features and labels\n",
    "X = pd.DataFrame(data['features'].tolist())  # Features are in list form\n",
    "y = data['Label']  # Labels: 1 for phishing, 0 for non-phishing\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Check accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Function to classify a new URL\n",
    "def classify_url(url):\n",
    "    features = extract_features(url)\n",
    "    prediction = model.predict([features])\n",
    "    if prediction[0] == 1:\n",
    "        return f\"The URL '{url}' is likely a phishing site.\"\n",
    "    else:\n",
    "        return f\"The URL '{url}' seems safe.\"\n",
    "\n",
    "# Test the classification with a user-provided URL\n",
    "user_url = input(\"Enter a URL to check for phishing: \")\n",
    "print(classify_url(user_url))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4225add4-38b2-465e-b91c-bef47c5efd95",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_file.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load data from CSV and display the first few rows\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_file.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData preview:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_file.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data from CSV and display the first few rows\n",
    "data = pd.read_csv('your_file.csv')\n",
    "print(\"Data preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Strip any leading/trailing spaces from column names\n",
    "data.columns = data.columns.str.strip()\n",
    "\n",
    "# Check if necessary columns exist\n",
    "required_columns = ['url', 'status']  # 'status' is the target variable\n",
    "missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "# Features and target variable\n",
    "X = data.drop(['status', 'url'], axis=1)  # Drop 'status' and 'url' from features\n",
    "y = data['status']  # 'status' is the target variable (1 = phishing, 0 = non-phishing)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)  # 100 trees in the forest\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Function to classify a new URL based on extracted features\n",
    "def classify_url(url):\n",
    "    # Create a dictionary of features with default values\n",
    "    default_features = {\n",
    "        'length_url': 0,\n",
    "        'length_hostname': 0,\n",
    "        'ip': 0,\n",
    "        'nb_dots': 0,\n",
    "        'nb_hyphens': 0,\n",
    "        # Add the rest of the features with default or calculated values\n",
    "        # ...\n",
    "    }\n",
    "    \n",
    "    # Assume a simple extraction logic for length_url\n",
    "    default_features['length_url'] = len(url)\n",
    "\n",
    "    # Extract specific features from the given URL (as per your headers)\n",
    "    # Example: number of dots in the URL\n",
    "    default_features['nb_dots'] = url.count('.')\n",
    "\n",
    "    # Transform the dictionary into a DataFrame to match the training data structure\n",
    "    feature_vector = pd.DataFrame([default_features])\n",
    "\n",
    "    # Predict using the trained model\n",
    "    prediction = model.predict(feature_vector)\n",
    "\n",
    "    return prediction[0]  # 1 = phishing, 0 = non-phishing\n",
    "\n",
    "# Test the classification with a user-provided URL\n",
    "user_url = input(\"Enter a URL to check for phishing: \")\n",
    "is_phishing = classify_url(user_url)\n",
    "\n",
    "if is_phishing:\n",
    "    print(f\"The URL '{user_url}' is likely a phishing site.\")\n",
    "else:\n",
    "    print(f\"The URL '{user_url}' seems safe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce45be-b772-4abb-9aeb-16a507d195bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
